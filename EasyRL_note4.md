# EasyRL task4 笔记


### 简单总结

1. 深度Q网络（deep Q-network，DQN）
* 是指基于深度学习的Q学习算法，主要结合了价值函数近似与神经网络技术，并采用目标网络和经历回放的方法进行网络的训练
* 是基于价值的算法，在基于价值的算法里面，我们学习的不是策略，而是评论员
* 评论员1：状态价值函数，有两种不同的方法衡量
	* 输入状态，根据状态算出以后的期望的累积奖励（expected accumulated reward）
	* 蒙特卡洛：是一个回归问题，游戏结束才能训练，方差很大，不常用
	* 时序差分：每步都可以训练，比较常用
* 评论员2：动作价值函数/Q函数
	* 输入状态-动作对，指在某一个状态采取某一个动作，假设我们都使用策略 π ，得到的累积奖励的期望值有多大
* 实现技巧
	* 目标网络（target network）：为了方便训练，把公式目标暂时固定，使训练变成回归的方式
	* 探索：避免只采取固定动作，可采用 ε-贪心和玻尔兹曼探索（Boltzmann exploration）
	* 经验回放（experience replay）：
		* 用回放缓冲区可以减少与环境交互的次数，节约时间；
		* 在训练网络的时候，其实我们希望一个批量里面的数据越多样（diverse）越好
* 进阶技巧
	* Double DQN：本来找 Q 值最大的 a 的时候，是用 Q′ 来算，是用目标网络来算，现在改成用另外一个会更新的 Q-network 来算
	* Dueling DQN：调整了一下神经网络架构，将原来的DQN的计算过程分为两个path
	* Prioritized Experience Replay：使用更好的经验去训练
	* Balance between MC and TD：每次多用几个步骤来训练
	* Noisy Net：在参数的空间上面加噪声，相当于有系统地在探索环境
	* Distributional Q-function：输出动作改成输出分布，然后再训练一个网络来选取风险更低的动作
	* Rainbow：全部技巧都实现
* 针对连续动作的 DQN
	* 可以采样出 N 个可能的 a，一个一个带到 Q函数里面，看谁最大
	* 把a当作是参数，然后要找一组a去最大化Q函数，就用梯度上升去更新 a 的值，最后看看能不能找到一个a去最大化Q函数
	* 设计网络：特别设计一个网络的架构，特别设计Q函数，使得解 arg max 的问题变得非常容易。也就是这边的Q函数不是一个一般的Q函数，特别设计一下它的样子，让你要找让这个Q函数最大的 a 的时候非常容易
	* 不使用DQN：基于策略的方法 PPO 和基于价值的方法 DQN，这两者其实是可以结合在一起的，也就是演员-评论员的方法
	
2. 演员-评论员算法
* 是一种结合策略梯度和时序差分学习的强化学习方法
* 演员：是指策略函数 πθ(a|s)，即学习一个策略以得到尽可能高的回报
* 评论员：是指价值函数 Vπ(s)，对当前策略的值函数进行估计，即评估演员的好坏
* 最知名的算法就是异步优势演员-评论员算法，如果去掉异步，则为优势演员-评论员（advantage actor-critic，A2C）算法
* 异步：多个网络并行训练
* 路径衍生策略梯度：路径衍生策略梯度里面，评论员会直接告诉演员采取什么样的动作才是好的；实际上算法上调整比较大，这块没看懂


### 关于项目二

1. 跑了一下领航员提供的 DQN 项目，reward 达到了 242.4，跑起来看了一下界面，小车虽然可以保持木棍平衡，但是往左跑出去了；
2. 我本来想尝试一下实现 Double DQN，~~但是我发现已经实现了，`algorithm.py` 里的 `predict` 已经是用不是目标 Q 函数来预测动作值，而是用会更新的 Q 网络来算的了~~
3. 把 max_episode 调到 3200 之后，reward 就能达到 500 了，跑起来能看到小车不会走出屏幕外面