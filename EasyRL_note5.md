# EasyRL task5 笔记


### 简单总结

1. 稀疏奖励
  * 如果环境中的 reward 非常 sparse，reinforcement learning 的问题就会变得非常的困难
  * Reward Shaping：Reward shaping 的意思是说环境有一个固定的 reward，它是真正的 reward，但是为了让 agent 学出来的结果是我们要的样子，我们刻意地设计了一些 reward 来引导我们的 agent
  * Curiosity: 增加 curiosity driven reward 机制，如果环境呈现出的 state 跟 agent 预测的不同，则奖励
  * feature extractor: 为了把无关的，不重要的预测差异过滤掉，而设计的机制；
  * Curriculum Learning: 从易到难训练 agent
  * Reverse Curriculum Generation: 给定目标状态 Sg，然后采样接近的状态 S1；从 S1 出发，如果能到达 Sg 则给予奖励；如此类推，还要把一些太难或太简单的状态去掉，换成其他适中的状态
  * Hierarchical RL: 分层强化学习是说，我们有好几个 agent。然后，有一些 agent 负责比较 high level 的东西，它负责订目标，然后它订完目标以后，再分配给其他的 agent，去把它执行完成

2. 模仿学习
  *  what: 在模仿学习里面，你有一些专家的示范，那机器也可以跟环境互动，但它没有办法从环境里面得到任何的奖励，它只能看着专家的示范来学习什么是好，什么是不好
  * 行为克隆：
    * 专家做什么，机器就做一模一样的事
    * 数据集聚合：收集更多样性的数据，而不是只收集专家所看到的观测
    * 问题：机器会完全模仿专家的行为，不管专家的行为是否有道理，就算没有道理，没有什么用的，就算这是专家本身的习惯，机器也会硬把它记下来
  * 逆强化学习: 
    * 奖励要从专家那边推出来，有了环境和专家示范以后，去反推出奖励函数长什么样子；最后进行强化学习；
    * 演员得到高分以后，我们就改奖励函数，仍然让专家可以得到比演员更高的分数，重复这个步骤；直到奖励函数没有办法分辨出谁应该会得到比较高的分数（跟 GAN 类似）
    * 有趣的地方：通常你不需要太多的训练数据；并且训练出来的智能体的表现风格不一样
  * 第三人称视角模仿学习
    * 用到了领域对抗训练（domain-adversarial Training）: 需要学习一个特征提取器，机器在第三人称的时候跟它在第一人称的时候看到的视野其实是一样的，就是把最重要的东西抽出来就好了
  * 序列生成和聊天机器人
    * 每一个词汇其实就是一个动作。你让机器做句子生成的时候，其实就是在模仿专家的轨迹；
    * 如果我们单纯用最大似然（maximum likelihood）这个技术来最大化会得到似然（likelihood），这个其实就是行为克隆
    * 把逆强化学习的技术用于句子生成或聊天机器人，其实就是序列生成对抗网络跟它的种种的变形
